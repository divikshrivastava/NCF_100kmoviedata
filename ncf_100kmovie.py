# -*- coding: utf-8 -*-
"""NCF_100kmovie.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dFLyTIJ8-2lJbGm6W8Nnk-zZGLndZPD-
"""

!pip install pyspark

import os
import numpy as np
from pyspark.sql import SparkSession
from pyspark import SparkContext
import pandas as pd
from pyspark.mllib.recommendation import ALS
import math
import pyspark.sql
from pyspark.sql.functions import udf
from pyspark.sql.types import *
from pyspark.ml.evaluation import RegressionEvaluator
import time

# import os
# import numpy as np
# from pyspark.sql import SparkSession
# from pyspark import SparkContext
# import pandas as pd
# from pyspark.mllib.recommendation import ALS
# import math
# import pyspark.sql
# from pyspark.sql.functions import udf
# from pyspark.sql.types import *
# from pyspark.ml.evaluation import RegressionEvaluator
# import time

# # Calling spark session to register application
# spark = SparkSession \
#     .builder \
#     .appName("Recom") \
#     .config("spark.recom.demo", "1") \
#     .getOrCreate()
# # lambda word: (word, 1)

# ratings_df = spark.read \
#     .format("csv") \
#     .option("header", "true") \
#     .option("inferSchema", "true") \
#     .load("train_ratings.csv")

# ratings_df = ratings_df[ratings_df.movieRating != 0.0]
# ratings_df.summary
# ratings_df.count()

# ratings_df = ratings_df[ratings_df.movieRating != 0.0]

# (trainingData,validationData,testData) = ratings_df.randomSplit([0.6,0.2,0.2],5) # randomSplit(weights, seed)

# Calling spark session to register application
spark = SparkSession \
    .builder \
    .appName("Recom") \
    .config("spark.recom.demo", "1") \
    .getOrCreate()
# lambda word: (word, 1)

ratings_df = spark.read \
    .format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("train_ratings.csv")

ratings_df = ratings_df.drop('') # reduntant> no values are dropped
ratings_df.count()

ratings_df = ratings_df[ratings_df.movieRating != 0.0]
ratings_df.summary
ratings_df.count()

(trainingData,validationData,testData) = ratings_df.randomSplit([0.6,0.2,0.2],5) # randomSplit(weights, seed)

validation_for_predict = validationData.select('userid','movieid')
test_for_predict = testData.select('userid','movieid')

"""Model training"""

!pip install tensorflow

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import RootMeanSquaredError
import time

# Load the data (assuming it's already preprocessed)
ratings_df = pd.read_csv("train_ratings.csv")

# Define the neural collaborative filtering model
def create_ncf_model(user_dim, movie_dim):
    user_input = Input(shape=(1,))
    movie_input = Input(shape=(1,))

    user_embedding = Embedding(user_dim+1, 10)(user_input)
    movie_embedding = Embedding(movie_dim+1, 10)(movie_input)

    user_flatten = Flatten()(user_embedding)
    movie_flatten = Flatten()(movie_embedding)

    concat = Concatenate()([user_flatten, movie_flatten])
    dense1 = Dense(64, activation='relu')(concat)
    dense2 = Dense(32, activation='relu')(dense1)
    output = Dense(1)(dense2)

    model = Model(inputs=[user_input, movie_input], outputs=output)
    model.compile(loss='mean_squared_error', optimizer=Adam(), metrics=[RootMeanSquaredError()])

    return model

def getNCFRecommendations(user,ratings_df,model, k): #

    userDf = ratings_df.filter(ratings_df.userid == user)
    mov = ratings_df.select('movieid').subtract(userDf.select('movieid'))

    # Convert user and movie IDs to numpy arrays
    user_array = np.full(mov.count(), user)
    movie_array = np.array(mov.select('movieid').rdd.flatMap(lambda x: x).collect())

    # Use the NCF model to predict ratings for unrated movies
    predictions = model.predict([user_array, movie_array]).flatten()

    # Combine user, movie, and prediction into a list of tuples
    recommendations = list(zip(user_array, movie_array, predictions))

    # Sort by predicted ratings in descending order and select the top k
    recommendations = sorted(recommendations, key=lambda x: x[2], reverse=True)[:k]

    # Set predictions to 0 if they are less than 3.8
    recommendations = [(user, movie, pred if pred >= 3.8 else 0) for user, movie, pred in recommendations]

    return recommendations

def recomendNCFMoviesForAllUsers(test_users,itemKs,testData,model):
    output_file = open("test_recommendations_ncf.txt", "a")
    print("total", len(test_users))
    wer = 0
    for u in range(len(test_users)):
      print(wer)
      wer=wer+1
      user = test_users[u]
      for userk in userKs:
        for itemk in itemKs:
            derived_rec = getNCFRecommendations(user,testData,model,itemk)
            output_file.write(str(user)+", ")
            # output_file.write(str(userk)+", ")
            output_file.write(str(itemk)+", ")
            for i in range(len(derived_rec)):
                output_file.write(str(derived_rec[i][1])+", ")
            output_file.write("\n")


user_dim = ratings_df['userid'].nunique()
movie_dim = ratings_df['movieid'].nunique()

# Prepare data for training
train_user = trainingData.select('userid').rdd.flatMap(lambda x: x).collect()
train_movie = trainingData.select('movieid').rdd.flatMap(lambda x: x).collect()
train_rating = trainingData.select('movieRating').rdd.flatMap(lambda x: x).collect()

test_user = testData.select('userid').rdd.flatMap(lambda x: x).collect()
test_movie = testData.select('movieid').rdd.flatMap(lambda x: x).collect()
test_rating = testData.select('movieRating').rdd.flatMap(lambda x: x).collect()


# Create and train the model
ncf_model = create_ncf_model(user_dim, movie_dim)

start_time = time.time()
ncf_model.fit([np.array(train_user), np.array(train_movie)], np.array(train_rating), epochs=10, batch_size=64)
end_time = time.time()
print(f"Time taken for training: {end_time - start_time} seconds")

"""Predictions"""

test_data = pd.read_csv('test_ratings.csv')
test_users = test_data.userid.unique()
print(len(test_users))
test_users_list = test_users.tolist()
userKs = [5] # 10,20,25 final run
itemKs = [5,10,25,50,100] # 5, 10, 25, 50 ,100
len(test_users_list)
start = time.time()
derived_rec = recomendNCFMoviesForAllUsers(test_users_list,itemKs,testData,ncf_model)
end = time.time()
print(end - start)

"""Precision, Recall and F1 Score calculation"""

def get_test_rec_movieIds_ncf(userId):
    user_df = test_data[test_data.userid == userId]
    user_movie_df = user_df[user_df.movieRating > 3.0]
    original_movieIds = user_movie_df.movieid.unique()
    return list(original_movieIds)


# get_test_rec_movieIds


count = 0
input_file = open('test_recommendations_ncf.txt','r')

userIds = []
actual_movies_watched = []
recomendations = []
common_movies = []
precision = []
recall = []
f1score = []

while (True):
    line = input_file.readline()
    if not line:
        break
    count +=1
    values = line.split(", ")
    userId = values[0]
    no_recomendations = values[1]
    recomendations_for_user = values[2:]

    recomendations_for_user.pop() ## removing last null(\n) value

    actual_movies_watched_by_users = get_test_rec_movieIds_ncf(int(userId)) # movies in test data
#     print(count)
    common_count = find_common(actual_movies_watched_by_users, recomendations_for_user) # movies in algo's predictions

#     print(userId,userK,no_recomendations)
#     print(common_count)

    userIds.append(userId)
    actual_movies_watched.append(len(actual_movies_watched_by_users))
    recomendations.append(int(no_recomendations))
    common_movies.append(common_count)

    total_count_for_precision = int(no_recomendations)
    total_count_for_recall = len(actual_movies_watched_by_users)

    temp_Precision = getPrecision(total_count_for_precision, common_count)
    precision.append(temp_Precision)
    temp_recall = getRecall(total_count_for_recall, common_count)
    recall.append(temp_recall)
    f1score.append(getf1score(temp_Precision, temp_recall))

print(count)

result_df_dict_ncf = {"userId":userIds,"actual_movies_watched":actual_movies_watched,
                  "recomendations":recomendations,"common_movies":common_movies,
                   "precision": precision, "recall": recall, "f1_score": f1score}

result_ncf_df = pd.DataFrame(result_df_dict_ncf)
result_ncf_df.head(25)
result_ncf_df = result_ncf_df.astype({"userId": int})
result_ncf_df

result_ncf_df_5 = result_ncf_df.loc[result_ncf_df['recomendations']==5]
result_ncf_df_10 = result_ncf_df.loc[result_ncf_df['recomendations']==10]
result_ncf_df_25 = result_ncf_df.loc[result_ncf_df['recomendations']==25] # final run
result_ncf_df_50 = result_ncf_df.loc[result_ncf_df['recomendations']==50]
result_ncf_df_100 = result_ncf_df.loc[result_ncf_df['recomendations']==100]

result_ncf_df_5.mean()

result_ncf_df_10.mean()

result_ncf_df_25.mean()

result_ncf_df_50.mean()

result_ncf_df_100.mean()